//
//  ChatVM.swift
//  Airchat
//
//  Created by æ¨é£ on 2025/6/18.
//

import Foundation
import SwiftUI
import Combine
import AVFoundation
import Speech

@MainActor
final class ChatVM: NSObject, ObservableObject {
    @Published var messages: [ChatMessage] = []
    @Published var composing = ""
    @Published var selectedImages: [AttachedImage] = []
    @Published var isLoading = false
    @Published var lastMessageUpdateTime = Date()
    @Published var showModelSelection = false
    @Published var shouldScrollToBottom = false
    @Published var showFileImporter = false
    @Published var animatingImageIDs = Set<UUID>()
    @Published var showAPIKeyInput = false
    @Published var showClearConfirmation = false
    @Published var isWebSearchEnabled = false // è”ç½‘æœç´¢å¼€å…³çŠ¶æ€

    // è¯­éŸ³è½¬æ–‡æœ¬ç›¸å…³
    @Published var isRecording = false
    @Published var isProcessingVoice = false
    @Published var speechRecognitionMethod: SpeechRecognitionMethod = .geminiAPI  // é»˜è®¤ä½¿ç”¨Gemini APIï¼ˆæ›´ç¨³å®šï¼‰

    // éŸ³é¢‘å½•åˆ¶ç›¸å…³
    private var audioRecorder: AVAudioRecorder?
    private var recordingURL: URL?
    
    // Apple Speech Recognition ç›¸å…³
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine: AVAudioEngine?
    
    private let api = ArkChatAPI()
    private let geminiAPI = GeminiOfficialAPI()
    private var scrollUpdateTimer: Timer?
    private let pasteboardMonitor = PasteboardMonitor()
    let modelConfig = ModelConfig()
    private let webSearchService = WebSearchService.shared
    
    // åŒé‡æ»šåŠ¨æœºåˆ¶ï¼šæµå¼è¾“å‡ºå®æ—¶æ»šåŠ¨ + æ™®é€šé˜²æŠ–æ»šåŠ¨
    private let streamingScrollSubject = PassthroughSubject<Void, Never>()
    private let normalScrollSubject = PassthroughSubject<Void, Never>()
    
    // æµå¼è¾“å‡ºæ—¶çš„å®æ—¶æ»šåŠ¨ï¼ˆæ— é˜²æŠ–ï¼‰
    var streamingScrollPublisher: AnyPublisher<Void, Never> {
        streamingScrollSubject.eraseToAnyPublisher()
    }
    
    // æ™®é€šæƒ…å†µä¸‹çš„é˜²æŠ–æ»šåŠ¨ - ä¼˜åŒ–ç‰ˆæœ¬
    var normalScrollPublisher: AnyPublisher<Void, Never> {
        normalScrollSubject
            .throttle(for: .milliseconds(30), scheduler: DispatchQueue.main, latest: true) // å‡å°‘é˜²æŠ–æ—¶é—´
            .eraseToAnyPublisher()
    }
    
    // è®¡ç®—å±æ€§ï¼šè·å–æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯çš„å†…å®¹æ–‡æœ¬
    var lastAssistantMessageText: String {
        if let lastMessage = messages.last(where: { $0.role == .assistant }) {
            return lastMessage.content.displayText
        }
        return ""
    }
    
    // æ‰“å­—æœºæ•ˆæœç›¸å…³
    private var pendingTokens = ""
    private var typewriterTimer: Timer?
    private let typewriterSpeed: TimeInterval = 0.02 // 20msæ¯å­—ç¬¦ï¼Œæ‰“å­—æœºé€Ÿåº¦
    private var isTypewriting = false
    
    // ç§»é™¤äº†æ»šåŠ¨èŠ‚æµï¼Œæ‰“å­—æœºæ¨¡å¼éœ€è¦å®æ—¶è·Ÿéš
    
    override init() {
        // åˆå§‹åŒ–ç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«å½“å‰æ—¥æœŸ
        let dateFormatter = DateFormatter()
        dateFormatter.locale = Locale(identifier: "zh_CN")
        dateFormatter.dateFormat = "yyyyå¹´MMæœˆddæ—¥"
        let currentDate = dateFormatter.string(from: Date())
        
        let systemMessage = """
        ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚å½“å‰æ—¥æœŸæ˜¯\(currentDate)ã€‚
        
        é‡è¦æç¤ºï¼š
        - å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æˆ–å…¶ä»–æ—¶æ•ˆæ€§ä¿¡æ¯æ—¶ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œè¯·ä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸï¼ˆ\(currentDate)ï¼‰ã€‚
        - ä¸è¦ä½¿ç”¨å†å²æ—¥æœŸï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ã€‚
        """
        
        messages = [ChatMessage(role: .system, content: systemMessage)]
    }

    // MARK: - éŸ³é¢‘å½•åˆ¶åŠŸèƒ½

    // è¯­éŸ³å½•åˆ¶æ§åˆ¶æ–¹æ³•
    func toggleVoiceRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    // åˆ‡æ¢è¯­éŸ³è¯†åˆ«æ–¹æ³•
    func switchSpeechRecognitionMethod() {
        // åœæ­¢å½“å‰å½•éŸ³
        if isRecording {
            stopRecording()
        }
        
        speechRecognitionMethod = speechRecognitionMethod == .appleSpeech ? .geminiAPI : .appleSpeech
        print("ğŸ¤ åˆ‡æ¢è¯­éŸ³è¯†åˆ«æ–¹æ³•ä¸º: \(speechRecognitionMethod.displayName)")
        
        // æ˜¾ç¤ºåˆ‡æ¢æç¤º
        let methodName = speechRecognitionMethod == .appleSpeech ? "Appleè¯­éŸ³è¯†åˆ«" : "Gemini AIè¯†åˆ«"
        composing = "å·²åˆ‡æ¢åˆ°\(methodName)"
        
        // 2ç§’åæ¸…ç©ºæç¤º
        DispatchQueue.main.asyncAfter(deadline: .now() + 2) {
            if self.composing == "å·²åˆ‡æ¢åˆ°\(methodName)" {
                self.composing = ""
            }
        }
    }

    private func startRecording() {
        switch speechRecognitionMethod {
        case .appleSpeech:
            startAppleSpeechRecognition()
        case .geminiAPI:
            beginRecording()  // ä½¿ç”¨åŸæœ‰çš„å½•éŸ³+Gemini APIæ–¹å¼
        }
    }

    private func beginRecording() {
        // åˆ›å»ºå½•éŸ³æ–‡ä»¶URL
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let audioFilename = documentsPath.appendingPathComponent("recording_\(Date().timeIntervalSince1970).wav")
        recordingURL = audioFilename

        // è®¾ç½®å½•éŸ³å‚æ•° - ä¸ºè¯­éŸ³è¯†åˆ«ä¼˜åŒ–
        let settings: [String: Any] = [
            AVFormatIDKey: Int(kAudioFormatLinearPCM),  // ä½¿ç”¨ WAV æ ¼å¼ï¼Œæ›´é€‚åˆè¯­éŸ³è¯†åˆ«
            AVSampleRateKey: 16000,  // 16kHz æ˜¯è¯­éŸ³è¯†åˆ«çš„æ ‡å‡†é‡‡æ ·ç‡
            AVNumberOfChannelsKey: 1,  // å•å£°é“
            AVLinearPCMBitDepthKey: 16,  // 16ä½æ·±åº¦
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsFloatKey: false,
            AVEncoderAudioQualityKey: AVAudioQuality.max.rawValue  // ä½¿ç”¨æœ€é«˜è´¨é‡ç¡®ä¿æ¸…æ™°åº¦
        ]

        do {
            audioRecorder = try AVAudioRecorder(url: audioFilename, settings: settings)
            audioRecorder?.isMeteringEnabled = true  // å¯ç”¨éŸ³é‡ç›‘æµ‹
            audioRecorder?.record()

            isRecording = true
            print("ğŸ¤ å¼€å§‹å½•éŸ³: \(audioFilename.lastPathComponent)")
            print("ğŸ¤ å½•éŸ³è®¾ç½®: WAVæ ¼å¼, 16kHzé‡‡æ ·ç‡, 16ä½æ·±åº¦")
        } catch {
            print("âŒ å½•éŸ³å¤±è´¥: \(error)")
        }
    }

    private func stopRecording() {
        switch speechRecognitionMethod {
        case .appleSpeech:
            stopAppleSpeechRecognition()
        case .geminiAPI:
            audioRecorder?.stop()
            isRecording = false
            isProcessingVoice = true
            print("ğŸ¤ åœæ­¢å½•éŸ³")

            // å¼€å§‹å¤„ç†éŸ³é¢‘
            if let url = recordingURL {
                Task {
                    await processAudioFile(url)
                }
            }
        }
    }
    
    // MARK: - Apple Speech Recognition
    
    private func startAppleSpeechRecognition() {
        // æ£€æŸ¥æƒé™
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            Task { @MainActor in
                guard let self = self else { return }
                
                switch authStatus {
                case .authorized:
                    self.beginAppleSpeechRecognition()
                case .denied, .restricted, .notDetermined:
                    print("âŒ è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»")
                    self.composing = "è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­å¼€å¯"
                    self.isRecording = false
                    self.isProcessingVoice = false
                @unknown default:
                    print("âŒ æœªçŸ¥çš„æƒé™çŠ¶æ€")
                    self.isRecording = false
                    self.isProcessingVoice = false
                }
            }
        }
    }
    
    private func beginAppleSpeechRecognition() {
        // åœæ­¢ä»»ä½•ç°æœ‰çš„è¯†åˆ«ä»»åŠ¡
        stopAppleSpeechRecognition()
        
        // æ£€æŸ¥è¯­éŸ³è¯†åˆ«æ˜¯å¦å¯ç”¨
        var targetRecognizer: SFSpeechRecognizer?
        
        // å…ˆå°è¯•ä¸­æ–‡
        if let chineseRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN")), chineseRecognizer.isAvailable {
            targetRecognizer = chineseRecognizer
            print("ğŸ¤ ä½¿ç”¨ä¸­æ–‡è¯­éŸ³è¯†åˆ«")
        }
        // å†å°è¯•è‹±æ–‡
        else if let englishRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US")), englishRecognizer.isAvailable {
            targetRecognizer = englishRecognizer
            print("ğŸ¤ ä½¿ç”¨è‹±æ–‡è¯­éŸ³è¯†åˆ«")
        }
        // éƒ½ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°Gemini
        else {
            print("âŒ è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°Gemini API")
            speechRecognitionMethod = .geminiAPI
            beginRecording()
            return
        }
        
        self.speechRecognizer = targetRecognizer
        
        do {
            // è®¾ç½®éŸ³é¢‘å¼•æ“
            audioEngine = AVAudioEngine()
            guard let audioEngine = audioEngine else { 
                throw TranscriptionError.speechRecognitionNotAvailable
            }
            
            let inputNode = audioEngine.inputNode
            
            // åˆ›å»ºè¯†åˆ«è¯·æ±‚
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else { 
                throw TranscriptionError.speechRecognitionNotAvailable
            }
            
            recognitionRequest.shouldReportPartialResults = true
            
            // å¼€å§‹è¯†åˆ«ä»»åŠ¡
            recognitionTask = self.speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                Task { @MainActor in
                    guard let self = self else { return }
                    
                    if let result = result {
                        // å®æ—¶æ›´æ–°è¯†åˆ«ç»“æœ
                        self.composing = result.bestTranscription.formattedString
                        
                        if result.isFinal {
                            print("ğŸ¤ è¯­éŸ³è¯†åˆ«å®Œæˆ: \(result.bestTranscription.formattedString)")
                            self.stopAppleSpeechRecognition()
                        }
                    }
                    
                    if let error = error {
                        print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                        self.stopAppleSpeechRecognition()
                    }
                }
            }
            
            // ç§»é™¤ç°æœ‰çš„tapï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            inputNode.removeTap(onBus: 0)
            
            // è®¾ç½®éŸ³é¢‘æ ¼å¼ - ä½¿ç”¨å®‰å…¨çš„æ ¼å¼
            let recordingFormat = AVAudioFormat(standardFormatWithSampleRate: 16000, channels: 1)
            guard let format = recordingFormat else {
                throw TranscriptionError.speechRecognitionNotAvailable
            }
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { [weak recognitionRequest] buffer, _ in
                recognitionRequest?.append(buffer)
            }
            
            // å¯åŠ¨éŸ³é¢‘å¼•æ“
            audioEngine.prepare()
            try audioEngine.start()
            
            isRecording = true
            isProcessingVoice = false
            print("ğŸ¤ å¼€å§‹Appleè¯­éŸ³è¯†åˆ«...")
            
        } catch {
            print("âŒ å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥: \(error)")
            composing = "è¯­éŸ³è¯†åˆ«å¯åŠ¨å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°Gemini API"
            isRecording = false
            isProcessingVoice = false
            
            // è‡ªåŠ¨åˆ‡æ¢åˆ°Gemini API
            speechRecognitionMethod = .geminiAPI
            beginRecording()
        }
    }
    
    private func stopAppleSpeechRecognition() {
        // å®‰å…¨åœ°åœæ­¢éŸ³é¢‘å¼•æ“
        if let audioEngine = audioEngine {
            if audioEngine.isRunning {
                audioEngine.stop()
            }
            // å®‰å…¨åœ°ç§»é™¤tap
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // ç»“æŸè¯†åˆ«è¯·æ±‚
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // å–æ¶ˆè¯†åˆ«ä»»åŠ¡
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // æ¸…ç†éŸ³é¢‘å¼•æ“
        audioEngine = nil
        
        // æ¸…ç†çŠ¶æ€
        isRecording = false
        isProcessingVoice = false
        
        print("ğŸ¤ åœæ­¢Appleè¯­éŸ³è¯†åˆ«")
    }
    
    func send() {
        guard !composing.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty || !selectedImages.isEmpty else { return }
        
        // Check if API key is set
        if KeychainHelper.shared.apiKey == nil || KeychainHelper.shared.apiKey?.isEmpty == true {
            showAPIKeyInput = true
            return
        }
        
        let messageContent: MessageContent
        if selectedImages.isEmpty {
            messageContent = .text(composing)
        } else {
            var contentParts: [ContentPart] = []
            
            // Add text if present
            if !composing.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
                contentParts.append(.text(composing))
            }
            
            // Add images
            for image in selectedImages {
                contentParts.append(.imageUrl(image))
            }
            
            messageContent = .multimodal(contentParts)
        }
        
        let userMessage = ChatMessage(role: .user, content: messageContent)
        messages.append(userMessage)
        composing = ""
        selectedImages = []
        isLoading = true
        
        // Trigger scroll for user message
        lastMessageUpdateTime = Date()
        
        Task {
            do {
                let stream: AsyncThrowingStream<StreamingChunk, Error>
                
                // æ ¹æ®æ¨¡å‹æä¾›å•†é€‰æ‹©æ­£ç¡®çš„ API
                if modelConfig.selectedModel.provider == "Google Official" {
                    // ä½¿ç”¨å®˜æ–¹ Gemini APIï¼Œä¼ é€’å…·ä½“çš„æ¨¡å‹åç§°
                    let modelName = modelConfig.selectedModel.id.replacingOccurrences(of: "google-official/", with: "")
                    stream = try await geminiAPI.send(messages: messages, stream: true, model: modelName)
                } else {
                    // ä½¿ç”¨ OpenRouter API
                    let baseModelId = modelConfig.selectedModel.id
                    var actualModelId = baseModelId
                    
                    // å¦‚æœæ˜¯GPT-4oä¸”å¼€å¯äº†è”ç½‘ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°è”ç½‘ç‰ˆæœ¬
                    if baseModelId == "openai/gpt-4o" && isWebSearchEnabled {
                        actualModelId = "openai/gpt-4o:online"
                        print("ğŸ”§ [AUTO-SWITCH] GPT-4o â†’ GPT-4o:online (è”ç½‘æ¨¡å¼)")
                    }
                    
                    api.selectedModel = actualModelId
                    
                    // æ£€æŸ¥æ¨¡å‹ç±»å‹å†³å®šæœç´¢ç­–ç•¥
                    if actualModelId.contains(":online") || actualModelId.contains("search-preview") {
                        // è”ç½‘æ¨¡å‹ï¼šç›´æ¥å‘é€ï¼Œè‡ªåŠ¨è”ç½‘
                        stream = try await api.send(messages: messages, stream: true, enableWebSearch: false)
                    } else {
                        // ä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœå¯ç”¨è”ç½‘ï¼‰
                        let enableWebSearch = isWebSearchEnabled && supportsWebSearch
                        stream = try await api.send(messages: messages, stream: true, enableWebSearch: enableWebSearch)
                    }
                }
                
                for try await chunk in stream {
                    appendOrUpdateAssistant(chunk)
                }
                
                // ç¡®ä¿æœ€åçš„å­—ç¬¦éƒ½è¢«æ˜¾ç¤º
                flushRemainingCharacters()
                stopTypewriterEffect()
                isLoading = false
                
                // æœ€ç»ˆæ»šåŠ¨åˆ°åº•éƒ¨ - ç¡®ä¿åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œ
                Task { @MainActor in
                    triggerNormalScroll()
                }
            } catch {
                // ç¡®ä¿é”™è¯¯æƒ…å†µä¸‹ä¹Ÿæ˜¾ç¤ºæ‰€æœ‰å­—ç¬¦
                flushRemainingCharacters()
                stopTypewriterEffect()
                isLoading = false
                
                // Add error message to chat
                messages.append(ChatMessage(role: .assistant, content: "æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ï¼š\(error.localizedDescription)"))
                
                // æ»šåŠ¨åˆ°åº•éƒ¨æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯ - ç¡®ä¿åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œ
                Task { @MainActor in
                    triggerNormalScroll()
                }
            }
        }
    }
    
    private func appendOrUpdateAssistant(_ chunk: StreamingChunk) {
        // å¦‚æœæ²¡æœ‰åŠ©æ‰‹æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
        if messages.last?.role != .assistant {
            messages.append(ChatMessage(
                role: .assistant,
                content: "",
                reasoning: chunk.reasoning
            ))
        }
        
        // ç«‹å³å¤„ç†æ–°tokenï¼Œå¯åŠ¨æ‰“å­—æœºæ•ˆæœ
        if let content = chunk.content {
            pendingTokens += content
            startTypewriterEffect()
        }
        
        // å¤„ç†reasoningï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰
        if let reasoning = chunk.reasoning {
            if messages[messages.count - 1].reasoning == nil {
                messages[messages.count - 1].reasoning = ""
            }
            messages[messages.count - 1].reasoning! += reasoning
            
            // æ¨ç†è¿‡ç¨‹æ›´æ–°æ—¶ä¹Ÿéœ€è¦æ»šåŠ¨è·Ÿéš
            triggerStreamingScroll()
        }
        
        // å¤„ç†å·¥å…·è°ƒç”¨
        if let toolCalls = chunk.toolCalls {
            // å°†tool_callsä¿¡æ¯æ·»åŠ åˆ°æœ€åä¸€æ¡assistantæ¶ˆæ¯ä¸­
            if let lastIndex = messages.lastIndex(where: { $0.role == .assistant }) {
                messages[lastIndex].toolCalls = toolCalls
                print("ğŸ”§ Added \(toolCalls.count) tool calls to assistant message")
            }
            
            Task {
                await handleToolCalls(toolCalls)
            }
        }
    }
    
    // å¯åŠ¨æ‰“å­—æœºæ•ˆæœ
    private func startTypewriterEffect() {
        guard !isTypewriting && !pendingTokens.isEmpty else { return }
        
        isTypewriting = true
        typewriterTimer?.invalidate()
        
        typewriterTimer = Timer.scheduledTimer(withTimeInterval: typewriterSpeed, repeats: true) { [weak self] _ in
            Task { @MainActor in
                self?.typeNextCharacter()
            }
        }
    }
    
    // æ‰“å­—æœºé€å­—ç¬¦æ˜¾ç¤º
    private func typeNextCharacter() {
        guard !pendingTokens.isEmpty else {
            stopTypewriterEffect()
            return
        }
        
        // å–å‡ºç¬¬ä¸€ä¸ªå­—ç¬¦
        let nextChar = String(pendingTokens.removeFirst())
        
        // ç«‹å³æ›´æ–°UIæ˜¾ç¤ºå­—ç¬¦
        if let lastIndex = messages.lastIndex(where: { $0.role == .assistant }) {
            switch messages[lastIndex].content {
            case .text(let existingText):
                messages[lastIndex].content = .text(existingText + nextChar)
            case .multimodal(let parts):
                var updatedParts = parts
                if let lastTextIndex = updatedParts.lastIndex(where: { 
                    if case .text = $0 { return true }
                    return false 
                }) {
                    if case .text(let existingText) = updatedParts[lastTextIndex] {
                        updatedParts[lastTextIndex] = .text(existingText + nextChar)
                    }
                } else {
                    updatedParts.append(.text(nextChar))
                }
                messages[lastIndex].content = .multimodal(updatedParts)
            }
        }
        
        // æ‰“å­—æœºæ¨¡å¼ä¸‹éœ€è¦å®æ—¶æ»šåŠ¨è·Ÿéš
        triggerStreamingScroll()
    }
    
    // è§¦å‘æµå¼è¾“å‡ºæ—¶çš„å®æ—¶æ»šåŠ¨
    private func triggerStreamingScroll() {
        streamingScrollSubject.send()
    }
    
    // è§¦å‘æ™®é€šæƒ…å†µä¸‹çš„é˜²æŠ–æ»šåŠ¨
    private func triggerNormalScroll() {
        normalScrollSubject.send()
    }
    
    // åœæ­¢æ‰“å­—æœºæ•ˆæœ
    private func stopTypewriterEffect() {
        isTypewriting = false
        typewriterTimer?.invalidate()
        typewriterTimer = nil
    }
    
    // ç«‹å³å®Œæˆæ‰€æœ‰å‰©ä½™å­—ç¬¦ï¼ˆç”¨äºå¿«é€Ÿå®Œæˆï¼‰
    private func flushRemainingCharacters() {
        guard !pendingTokens.isEmpty else { return }
        
        if let lastIndex = messages.lastIndex(where: { $0.role == .assistant }) {
            switch messages[lastIndex].content {
            case .text(let existingText):
                messages[lastIndex].content = .text(existingText + pendingTokens)
            case .multimodal(let parts):
                var updatedParts = parts
                if let lastTextIndex = updatedParts.lastIndex(where: { 
                    if case .text = $0 { return true }
                    return false 
                }) {
                    if case .text(let existingText) = updatedParts[lastTextIndex] {
                        updatedParts[lastTextIndex] = .text(existingText + pendingTokens)
                    }
                } else {
                    updatedParts.append(.text(pendingTokens))
                }
                messages[lastIndex].content = .multimodal(updatedParts)
            }
        }
        
        pendingTokens = ""
        triggerNormalScroll()
    }
    
    func clearChat() {
        // é‡æ–°åˆ›å»ºå¸¦æœ‰å½“å‰æ—¥æœŸçš„ç³»ç»Ÿæ¶ˆæ¯
        let dateFormatter = DateFormatter()
        dateFormatter.locale = Locale(identifier: "zh_CN")
        dateFormatter.dateFormat = "yyyyå¹´MMæœˆddæ—¥"
        let currentDate = dateFormatter.string(from: Date())
        
        let systemMessage = """
        ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚å½“å‰æ—¥æœŸæ˜¯\(currentDate)ã€‚
        
        é‡è¦æç¤ºï¼š
        - å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æˆ–å…¶ä»–æ—¶æ•ˆæ€§ä¿¡æ¯æ—¶ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œè¯·ä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸï¼ˆ\(currentDate)ï¼‰ã€‚
        - ä¸è¦ä½¿ç”¨å†å²æ—¥æœŸï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ã€‚
        """
        
        messages = [ChatMessage(role: .system, content: systemMessage)]
        
        // æ¸…ç†æ‰€æœ‰å®šæ—¶å™¨å’Œç¼“å†²åŒº
        scrollUpdateTimer?.invalidate()
        scrollUpdateTimer = nil
        stopTypewriterEffect()
        pendingTokens = ""
        shouldScrollToBottom = false
    }
    
    // åˆ‡æ¢è”ç½‘çŠ¶æ€
    func toggleWebSearch() {
        isWebSearchEnabled.toggle()
    }
    
    // æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦æ”¯æŒè”ç½‘æœç´¢
    var supportsWebSearch: Bool {
        let modelId = modelConfig.selectedModel.id
        
        // OpenRouterå†…ç½®è”ç½‘æ¨¡å‹
        if modelId.contains(":online") || 
           modelId.contains("search-preview") {
            return true
        }
        
        // æ”¯æŒè”ç½‘åŠŸèƒ½çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oè‡ªåŠ¨åˆ‡æ¢åˆ°:onlineç‰ˆæœ¬ï¼‰
        let webSearchModels = [
            "google/gemini-2.5-pro",
            "anthropic/claude-3.5-sonnet", 
            "openai/o4-mini-high",
            "openai/gpt-4o"  // æ”¯æŒé€šè¿‡è”ç½‘å¼€å…³è‡ªåŠ¨åˆ‡æ¢åˆ°:onlineç‰ˆæœ¬
        ]
        return webSearchModels.contains(modelId)
    }
    
    func handlePaste() {
        if let image = pasteboardMonitor.getImageFromPasteboard() {
            processImage(image)
        }
    }
    
    func handleDroppedImage(_ image: NSImage) {
        processImage(image)
    }
    
    func handleDroppedImageFile(at url: URL) {
        if let image = NSImage(contentsOf: url) {
            processImage(image)
        }
    }
    
    // å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚
    @MainActor
    private func handleToolCalls(_ toolCalls: [ToolCall]) async {
        print("ğŸ”§ [Step 1] Starting tool call processing with \(toolCalls.count) tool calls")
        
        for (index, toolCall) in toolCalls.enumerated() {
            print("ğŸ”§ [Step 2.\(index + 1)] Processing tool call: \(String(describing: toolCall))")
            
            if toolCall.function?.name == "web_search" {
                do {
                    print("ğŸ”§ [Step 3] Web search tool call detected")
                    print("ğŸ”§ Tool call ID: \(String(describing: toolCall.id))")
                    print("ğŸ”§ Tool call function: \(String(describing: toolCall.function))")
                    let arguments = toolCall.function?.arguments ?? ""
                    print("ğŸ”§ Tool call arguments (raw): '\(arguments)'")
                    
                    var query: String? = nil
                    
                    // å°è¯•ä¸åŒçš„è§£ææ–¹å¼
                    if let queryData = arguments.data(using: .utf8) {
                        // æ–¹å¼1: æ ‡å‡†JSONè§£æ
                        if let jsonObject = try? JSONSerialization.jsonObject(with: queryData) as? [String: Any],
                           let q = jsonObject["query"] as? String {
                            query = q
                            print("ğŸ”§ [Step 4a] Query parsed from JSON: '\(q)'")
                        } 
                        // æ–¹å¼2: å¯èƒ½æ˜¯ç›´æ¥çš„å­—ç¬¦ä¸²
                        else if arguments.trimmingCharacters(in: .whitespacesAndNewlines).first != "{" {
                            query = arguments.trimmingCharacters(in: .whitespacesAndNewlines)
                            print("ğŸ”§ [Step 4b] Query parsed as direct string: '\(query!)'")
                        }
                        // æ–¹å¼3: å°è¯•è§£ç ä¸ºå¸¦æœ‰ä¸åŒé”®åçš„JSON
                        else if let jsonObject = try? JSONSerialization.jsonObject(with: queryData) as? [String: Any] {
                            // å°è¯•å…¶ä»–å¯èƒ½çš„é”®å
                            query = jsonObject["q"] as? String ?? 
                                   jsonObject["search_query"] as? String ?? 
                                   jsonObject["text"] as? String
                            print("ğŸ”§ [Step 4c] Query parsed from alternative JSON keys: '\(String(describing: query))'")
                        } else {
                            print("ğŸ”§ [Step 4d] Failed to parse arguments as JSON or string")
                        }
                    } else {
                        print("ğŸ”§ [Step 4e] Failed to convert arguments to Data")
                    }
                    
                    if let query = query, !query.isEmpty {
                        print("ğŸ”§ [Step 5] Executing search with query: '\(query)'")
                        
                        // æ˜¾ç¤ºæœç´¢çŠ¶æ€
                        appendOrUpdateAssistant(StreamingChunk(
                            content: "\n\nğŸ” æ­£åœ¨æœç´¢: \(query)\n\n",
                            reasoning: nil,
                            thinking: nil,
                            toolCalls: nil
                        ))
                        
                        do {
                            // æ‰§è¡Œæœç´¢
                            print("ğŸ”§ [Step 6] Calling webSearchService.search()")
                            let searchResults = try await webSearchService.search(query: query)
                            print("ğŸ”§ [Step 7] Search completed successfully with \(searchResults.count) results")
                            
                            // æ ¼å¼åŒ–æœç´¢ç»“æœ
                            var resultText = "**æœç´¢ç»“æœ:**\n\n"
                            for (index, result) in searchResults.enumerated() {
                                resultText += "\(index + 1). **[\(result.title)](\(result.url))**\n"
                                resultText += "   \(result.snippet)\n\n"
                            }
                            
                            print("ğŸ”§ [Step 8] Formatted search results (length: \(resultText.count) chars)")
                            
                            // å°†æœç´¢ç»“æœæ·»åŠ ä¸ºå·¥å…·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                            let toolMessage = ChatMessage(
                                role: .tool, 
                                content: resultText,
                                toolCallId: toolCall.id
                            )
                            messages.append(toolMessage)
                            print("ğŸ”§ [Step 9] Added tool message to conversation history")
                            print("ğŸ”§ Total messages in conversation: \(messages.count)")
                            
                            // ç»§ç»­è°ƒç”¨ API è®© GPT-4o åˆ†ææœç´¢ç»“æœ
                            print("ğŸ”§ [Step 10] Starting continuation API call")
                            await continueConversationAfterToolCall()
                            print("ğŸ”§ [Step 11] Tool call processing completed successfully")
                        } catch let searchError {
                            print("ğŸ”§ [Step 6 ERROR] Search execution failed: \(searchError)")
                            print("ğŸ”§ Search error type: \(type(of: searchError))")
                            print("ğŸ”§ Search error localizedDescription: \(searchError.localizedDescription)")
                            throw searchError
                        }
                    } else {
                        let parseError = NSError(domain: "ChatVM", code: 1, userInfo: [NSLocalizedDescriptionKey: "æ— æ³•è§£ææœç´¢æŸ¥è¯¢å‚æ•°"])
                        print("ğŸ”§ [Step 5 ERROR] Query parsing failed: \(parseError)")
                        throw parseError
                    }
                } catch {
                    print("ğŸ”§ [TOOL CALL ERROR] Tool call processing failed: \(error)")
                    print("ğŸ”§ Error type: \(type(of: error))")
                    print("ğŸ”§ Error localizedDescription: \(error.localizedDescription)")
                    
                    // æœç´¢å¤±è´¥æ—¶æ˜¾ç¤ºé”™è¯¯å’Œè¯¦ç»†ä¿¡æ¯
                    let errorArguments = toolCall.function?.arguments ?? ""
                    appendOrUpdateAssistant(StreamingChunk(
                        content: "\nâŒ æœç´¢å¤±è´¥: \(error.localizedDescription)\nå‚æ•°: \(errorArguments)\n\n",
                        reasoning: nil,
                        thinking: nil,
                        toolCalls: nil
                    ))
                }
            } else {
                print("ğŸ”§ [Step 3 SKIP] Unknown tool function: \(String(describing: toolCall.function?.name))")
            }
        }
        
        print("ğŸ”§ [COMPLETE] All tool calls processed")
    }
    
    // å·¥å…·è°ƒç”¨åç»§ç»­å¯¹è¯
    @MainActor
    private func continueConversationAfterToolCall() async {
        print("ğŸ”§ [CONTINUE Step 1] Starting continuation API call")
        print("ğŸ”§ Current model: \(modelConfig.selectedModel.id)")
        print("ğŸ”§ Current provider: \(modelConfig.selectedModel.provider)")
        print("ğŸ”§ Messages in conversation before API call: \(messages.count)")
        
        // æ‰“å°æœ€è¿‘å‡ æ¡æ¶ˆæ¯ä»¥éªŒè¯å·¥å…·æ¶ˆæ¯å·²æ­£ç¡®æ·»åŠ 
        for (index, message) in messages.suffix(3).enumerated() {
            print("ğŸ”§ Message[\(messages.count - 3 + index)]: role=\(message.role), content_length=\(message.content.displayText.count), toolCallId=\(String(describing: message.toolCallId))")
        }
        
        do {
            let stream: AsyncThrowingStream<StreamingChunk, Error>
            
            // æ ¹æ®æ¨¡å‹æä¾›å•†é€‰æ‹©æ­£ç¡®çš„ API
            if modelConfig.selectedModel.provider == "Google Official" {
                print("ğŸ”§ [CONTINUE Step 2a] Using Google Official API")
                // ä½¿ç”¨å®˜æ–¹ Gemini API
                let modelName = modelConfig.selectedModel.id.replacingOccurrences(of: "google-official/", with: "")
                print("ğŸ”§ Model name for Gemini: \(modelName)")
                stream = try await geminiAPI.send(messages: messages, stream: true, model: modelName)
            } else {
                print("ğŸ”§ [CONTINUE Step 2b] Using OpenRouter API")
                // ä½¿ç”¨ OpenRouter API
                let baseModelId = modelConfig.selectedModel.id
                var actualModelId = baseModelId
                
                // å¦‚æœæ˜¯GPT-4oä¸”å¼€å¯äº†è”ç½‘ï¼Œç»§ç»­ä½¿ç”¨è”ç½‘ç‰ˆæœ¬
                if baseModelId == "openai/gpt-4o" && isWebSearchEnabled {
                    actualModelId = "openai/gpt-4o:online"
                    print("ğŸ”§ [CONTINUE] Using GPT-4o:online for continuation")
                }
                
                api.selectedModel = actualModelId
                print("ğŸ”§ API model set to: \(api.selectedModel)")
                
                // å·¥å…·è°ƒç”¨åç»§ç»­å¯¹è¯ï¼Œä¸éœ€è¦å†æ¬¡å¯ç”¨å·¥å…·
                print("ğŸ”§ [CONTINUE Step 3] Sending API request with webSearch disabled")
                stream = try await api.send(messages: messages, stream: true, enableWebSearch: false)
            }
            
            print("ğŸ”§ [CONTINUE Step 4] API stream created successfully, starting to process chunks")
            var chunkCount = 0
            
            for try await chunk in stream {
                chunkCount += 1
                print("ğŸ”§ [CONTINUE Step 5.\(chunkCount)] Processing chunk: content=\(String(describing: chunk.content)), reasoning=\(String(describing: chunk.reasoning))")
                appendOrUpdateAssistant(chunk)
            }
            
            print("ğŸ”§ [CONTINUE Step 6] Stream completed with \(chunkCount) chunks")
            
            // ç¡®ä¿æœ€åçš„å­—ç¬¦éƒ½è¢«æ˜¾ç¤º
            flushRemainingCharacters()
            stopTypewriterEffect()
            print("ğŸ”§ [CONTINUE Step 7] Typewriter effect stopped")
            
            // æœ€ç»ˆæ»šåŠ¨åˆ°åº•éƒ¨
            Task { @MainActor in
                triggerNormalScroll()
            }
            print("ğŸ”§ [CONTINUE Step 8] Continuation completed successfully")
        } catch {
            print("ğŸ”§ [CONTINUE ERROR] Continuation API call failed: \(error)")
            print("ğŸ”§ Error type: \(type(of: error))")
            print("ğŸ”§ Error localizedDescription: \(error.localizedDescription)")
            
            if let nsError = error as NSError? {
                print("ğŸ”§ NSError domain: \(nsError.domain)")
                print("ğŸ”§ NSError code: \(nsError.code)")
                print("ğŸ”§ NSError userInfo: \(nsError.userInfo)")
            }
            
            // ç¡®ä¿é”™è¯¯æƒ…å†µä¸‹ä¹Ÿæ˜¾ç¤ºæ‰€æœ‰å­—ç¬¦
            flushRemainingCharacters()
            stopTypewriterEffect()
            
            // Add error message to chat
            appendOrUpdateAssistant(StreamingChunk(
                content: "æŠ±æ­‰ï¼Œåˆ†ææœç´¢ç»“æœæ—¶å‘ç”Ÿäº†é”™è¯¯ï¼š\(error.localizedDescription)",
                reasoning: nil,
                thinking: nil,
                toolCalls: nil
            ))
            
            // æ»šåŠ¨åˆ°åº•éƒ¨æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
            Task { @MainActor in
                triggerNormalScroll()
            }
        }
    }
    
    private func processImage(_ nsImage: NSImage) {
        guard let tiffData = nsImage.tiffRepresentation,
              let bitmap = NSBitmapImageRep(data: tiffData) else { return }
        
        // Convert to PNG for better compatibility
        guard let pngData = bitmap.representation(using: .png, properties: [:]) else { return }
        
        // Check size and compress if needed
        let maxSize = 5 * 1024 * 1024 // 5MB
        let imageData: Data
        
        if pngData.count > maxSize {
            // Try JPEG compression
            let quality = Double(maxSize) / Double(pngData.count)
            if let jpegData = bitmap.representation(using: .jpeg, properties: [.compressionFactor: quality]) {
                imageData = jpegData
            } else {
                imageData = pngData
            }
        } else {
            imageData = pngData
        }
        
        // Create base64 data URL
        let base64String = imageData.base64EncodedString()
        let mimeType = pngData.count > maxSize ? "image/jpeg" : "image/png"
        let dataUrl = "data:\(mimeType);base64,\(base64String)"
        
        // Add to selected images
        let attachedImage = AttachedImage(url: dataUrl)
        Task { @MainActor in
            selectedImages.append(attachedImage)
        }
    }
    
    func removeImage(_ image: AttachedImage) {
        selectedImages.removeAll { $0.id == image.id }
    }
    
    func handleFileSelection(_ result: Result<[URL], Error>) {
        switch result {
        case .success(let urls):
            for url in urls {
                if url.startAccessingSecurityScopedResource() {
                    defer { url.stopAccessingSecurityScopedResource() }
                    
                    if let fileData = try? Data(contentsOf: url) {
                        // Check file size (limit to 20MB)
                        let maxSize = 20 * 1024 * 1024 // 20MB
                        if fileData.count > maxSize {
                            print("File too large: \(fileData.count) bytes (max: \(maxSize))")
                            continue
                        }
                        
                        let fileName = url.lastPathComponent
                        let pathExtension = url.pathExtension.lowercased()
                        
                        if pathExtension == "pdf" {
                            // Handle PDF file
                            let base64String = fileData.base64EncodedString()
                            let dataUrl = "data:application/pdf;base64,\(base64String)"
                            
                            let attachedFile = AttachedImage(
                                url: dataUrl,
                                fileType: .pdf,
                                fileName: fileName
                            )
                            addImageWithAnimation(attachedFile)
                        } else {
                            // Handle image file
                            if NSImage(data: fileData) != nil {
                                // Compress if needed
                                let compressedData = compressImageData(fileData, maxSize: 5 * 1024 * 1024) // 5MB max after compression
                                
                                // Convert to base64 for API
                                let base64String = compressedData.base64EncodedString()
                                let mimeType = getMimeType(from: url)
                                let dataUrl = "data:\(mimeType);base64,\(base64String)"
                                
                                let attachedImage = AttachedImage(
                                    url: dataUrl,
                                    fileType: .image,
                                    fileName: fileName
                                )
                                addImageWithAnimation(attachedImage)
                            }
                        }
                    }
                }
            }
        case .failure(let error):
            print("File selection failed: \(error)")
        }
    }

    // MARK: - å›¾ç‰‡å¤„ç†è¾…åŠ©æ–¹æ³•

    private func addImageWithAnimation(_ image: AttachedImage) {
        selectedImages.append(image)
        animatingImageIDs.insert(image.id)

        // Remove from animation set after delay
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
            self.animatingImageIDs.remove(image.id)
        }
    }

    private func compressImageData(_ data: Data, maxSize: Int) -> Data {
        guard let nsImage = NSImage(data: data) else { return data }

        // If already small enough, return original
        if data.count <= maxSize {
            return data
        }

        // Calculate compression quality
        let compressionRatio = Double(maxSize) / Double(data.count)
        let quality = min(max(compressionRatio, 0.1), 0.9) // Between 0.1 and 0.9

        // Create bitmap representation
        if let tiffData = nsImage.tiffRepresentation,
           let bitmap = NSBitmapImageRep(data: tiffData) {

            let properties: [NSBitmapImageRep.PropertyKey: Any] = [
                .compressionFactor: quality
            ]

            if let compressedData = bitmap.representation(using: .jpeg, properties: properties) {
                return compressedData
            }
        }

        return data
    }

    private func getMimeType(from url: URL) -> String {
        let pathExtension = url.pathExtension.lowercased()
        switch pathExtension {
        case "jpg", "jpeg":
            return "image/jpeg"
        case "png":
            return "image/png"
        case "gif":
            return "image/gif"
        case "webp":
            return "image/webp"
        default:
            return "image/jpeg" // Default fallback
        }
    }

    // MARK: - éŸ³é¢‘å¤„ç†

    private func processAudioFile(_ url: URL) async {
        print("ğŸ¤ å¼€å§‹å¤„ç†éŸ³é¢‘æ–‡ä»¶: \(url.lastPathComponent)")

        do {
            // æ£€æŸ¥æ–‡ä»¶å¤§å°
            let fileAttributes = try FileManager.default.attributesOfItem(atPath: url.path)
            let fileSize = fileAttributes[.size] as? Int64 ?? 0
            let maxSize: Int64 = 20 * 1024 * 1024  // 20MB é™åˆ¶
            
            print("ğŸ¤ éŸ³é¢‘æ–‡ä»¶å¤§å°: \(fileSize / 1024) KB")
            
            if fileSize > maxSize {
                print("âš ï¸ éŸ³é¢‘æ–‡ä»¶è¿‡å¤§: \(fileSize / 1024 / 1024) MBï¼Œè¶…è¿‡ 20MB é™åˆ¶")
                composing = "å½•éŸ³æ–‡ä»¶è¿‡å¤§ï¼Œè¯·ç¼©çŸ­å½•éŸ³æ—¶é•¿"
                isProcessingVoice = false
                try? FileManager.default.removeItem(at: url)
                return
            }

            // ä½¿ç”¨å†…ç½®çš„è¯­éŸ³è½¬æ–‡å­—åŠŸèƒ½
            let transcription = try await transcribeAudio(from: url)

            // å°†è½¬å½•ç»“æœæ·»åŠ åˆ°è¾“å…¥æ¡†
            if composing.isEmpty {
                composing = transcription
            } else {
                composing += " " + transcription
            }

            print("ğŸ¤ è¯­éŸ³è½¬å½•å®Œæˆ: \(transcription)")

        } catch {
            print("âŒ è¯­éŸ³è½¬å½•å¤±è´¥: \(error.localizedDescription)")

            // æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if composing.isEmpty {
                composing = "è¯­éŸ³è½¬å½•å¤±è´¥ï¼Œè¯·é‡è¯•"
            }
        }

        // æ¸…ç†çŠ¶æ€å’Œä¸´æ—¶æ–‡ä»¶
        isProcessingVoice = false
        try? FileManager.default.removeItem(at: url)
    }

    // MARK: - è¯­éŸ³è½¬æ–‡å­—åŠŸèƒ½

    private func transcribeAudio(from audioURL: URL) async throws -> String {
        // è·å–Google APIå¯†é’¥
        guard let apiKey = KeychainHelper.shared.googleApiKey, !apiKey.isEmpty else {
            throw TranscriptionError.missingAPIKey
        }

        print("ğŸ¤ å¼€å§‹çœŸå®çš„è¯­éŸ³è½¬å½•...")

        // è¯»å–éŸ³é¢‘æ–‡ä»¶æ•°æ®
        let audioData = try Data(contentsOf: audioURL)
        let base64Audio = audioData.base64EncodedString()

        // æ„å»ºGemini APIè¯·æ±‚
        let url = URL(string: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=\(apiKey)")!

        // é‡è¯•æœºåˆ¶é…ç½®
        let maxRetries = 3
        var retryCount = 0
        var lastError: Error?
        
        while retryCount < maxRetries {
            do {
                var request = URLRequest(url: url)
                request.httpMethod = "POST"
                request.setValue("application/json", forHTTPHeaderField: "Content-Type")
                request.timeoutInterval = 30.0  // 30ç§’è¶…æ—¶

                // æ„å»ºè¯·æ±‚ä½“
                let requestBody: [String: Any] = [
                    "contents": [
                        [
                            "parts": [
                                [
                                    "text": "Please transcribe this audio recording accurately. Rules: 1) Return ONLY the exact words spoken, 2) No explanations or descriptions, 3) Preserve the original language (Chinese/English/etc), 4) Include punctuation naturally, 5) If unclear, transcribe your best guess rather than noting uncertainty."
                                ],
                                [
                                    "inline_data": [
                                        "mime_type": "audio/wav",
                                        "data": base64Audio
                                    ]
                                ]
                            ]
                        ]
                    ],
                    "generationConfig": [
                        "temperature": 0.0,  // è®¾ä¸º0ä»¥è·å¾—æœ€ç¡®å®šçš„ç»“æœ
                        "topK": 1,  // åªé€‰æ‹©æœ€å¯èƒ½çš„token
                        "topP": 0.1,  // å‡å°‘éšæœºæ€§
                        "maxOutputTokens": 2000  // å¢åŠ è¾“å‡ºé•¿åº¦é™åˆ¶
                    ]
                ]

                request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)

                // å‘é€è¯·æ±‚
                let (data, response) = try await URLSession.shared.data(for: request)

                // æ£€æŸ¥HTTPå“åº”
                guard let httpResponse = response as? HTTPURLResponse else {
                    throw TranscriptionError.transcriptionFailed("æ— æ•ˆçš„å“åº”")
                }

                // å¤„ç†ä¸åŒçš„çŠ¶æ€ç 
                switch httpResponse.statusCode {
                case 200:
                    // æˆåŠŸï¼Œç»§ç»­è§£æ
                    break
                case 429:
                    // é€Ÿç‡é™åˆ¶ï¼Œéœ€è¦é‡è¯•
                    print("âš ï¸ APIé€Ÿç‡é™åˆ¶ (429)ï¼Œç­‰å¾…åé‡è¯•...")
                    lastError = TranscriptionError.transcriptionFailed("APIé€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
                    retryCount += 1
                    // æŒ‡æ•°é€€é¿ï¼š2^retryCount ç§’
                    let waitTime = pow(2.0, Double(retryCount))
                    try await Task.sleep(nanoseconds: UInt64(waitTime * 1_000_000_000))
                    continue
                case 503:
                    // æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œéœ€è¦é‡è¯•
                    print("âš ï¸ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ (503)ï¼Œç­‰å¾…åé‡è¯•...")
                    lastError = TranscriptionError.transcriptionFailed("æœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
                    retryCount += 1
                    let waitTime = pow(2.0, Double(retryCount))
                    try await Task.sleep(nanoseconds: UInt64(waitTime * 1_000_000_000))
                    continue
                default:
                    let errorMessage = String(data: data, encoding: .utf8) ?? "æœªçŸ¥é”™è¯¯"
                    print("âŒ APIé”™è¯¯ (\(httpResponse.statusCode)): \(errorMessage)")
                    throw TranscriptionError.transcriptionFailed("APIé”™è¯¯: \(httpResponse.statusCode)")
                }

                // è§£æå“åº”
                guard let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
                      let candidates = json["candidates"] as? [[String: Any]],
                      let firstCandidate = candidates.first,
                      let content = firstCandidate["content"] as? [String: Any],
                      let parts = content["parts"] as? [[String: Any]],
                      let firstPart = parts.first,
                      let text = firstPart["text"] as? String else {

                    let responseString = String(data: data, encoding: .utf8) ?? "æ— æ³•è§£æå“åº”"
                    print("âŒ å“åº”è§£æå¤±è´¥: \(responseString)")
                    throw TranscriptionError.transcriptionFailed("å“åº”è§£æå¤±è´¥")
                }

                let transcription = text.trimmingCharacters(in: .whitespacesAndNewlines)
                print("ğŸ¤ è½¬å½•æˆåŠŸ: \(transcription)")

                return transcription
                
            } catch {
                lastError = error
                if retryCount < maxRetries - 1 {
                    print("âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œå‡†å¤‡é‡è¯• (\(retryCount + 1)/\(maxRetries))...")
                    retryCount += 1
                } else {
                    // è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    throw lastError ?? error
                }
            }
        }
        
        throw lastError ?? TranscriptionError.transcriptionFailed("è½¬å½•å¤±è´¥")
    }

    deinit {
        scrollUpdateTimer?.invalidate()
        typewriterTimer?.invalidate()
        
        // æ¸…ç†è¯­éŸ³è¯†åˆ«èµ„æºï¼ˆåœ¨éä¸»çº¿ç¨‹ä¸­ï¼‰
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.finish()
        audioRecorder?.stop()
    }
}

// MARK: - è¯­éŸ³è¯†åˆ«æ–¹æ³•

enum SpeechRecognitionMethod: String, CaseIterable {
    case appleSpeech = "Apple Speech"
    case geminiAPI = "Gemini API"
    
    var displayName: String {
        return self.rawValue
    }
}

// MARK: - è¯­éŸ³è½¬å½•é”™è¯¯ç±»å‹

enum TranscriptionError: LocalizedError {
    case missingAPIKey
    case audioFileNotFound
    case transcriptionFailed(String)
    case speechRecognitionNotAvailable
    case permissionDenied

    var errorDescription: String? {
        switch self {
        case .missingAPIKey:
            return "ç¼ºå°‘Gemini APIå¯†é’¥"
        case .audioFileNotFound:
            return "éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"
        case .transcriptionFailed(let message):
            return "è½¬å½•å¤±è´¥: \(message)"
        case .speechRecognitionNotAvailable:
            return "è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨"
        case .permissionDenied:
            return "è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»"
        }
    }
}

